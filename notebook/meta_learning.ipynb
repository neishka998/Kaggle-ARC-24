{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Flatten, Input, Dense,  Reshape, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/Users/neishkasrivastava/Documents/Personal/Kaggle/ARC/arc-prize-2024/new_train_data.csv\")\n",
    "train_df['id_num'] = train_df['id'].str.extract(r'(^[a-zA-Z0-9]+)')\n",
    "train_df['input_shape'] = train_df['input_shape'].apply(lambda x: tuple(map(int, x.strip('()').split(','))))\n",
    "\n",
    "eval_df = pd.read_csv(\"/Users/neishkasrivastava/Documents/Personal/Kaggle/ARC/arc-prize-2024/new_eval_data.csv\")\n",
    "eval_df['id_num'] = eval_df['id'].str.extract(r'(^[a-zA-Z0-9]+)')\n",
    "\n",
    "display(train_df.head(30))\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping the training set based on tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_array(string):\n",
    "    return np.array(ast.literal_eval(string))\n",
    "\n",
    "def apply_padding(array, target_shape):\n",
    "    \"\"\"\n",
    "    Pads an array to the target shape with zeros.\n",
    "    \n",
    "    Parameters:\n",
    "    array (np.ndarray): The input array to pad.\n",
    "    target_shape (tuple): The target shape (rows, columns) to pad the array to.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: The padded array with the target shape.\n",
    "    \"\"\"\n",
    "    current_shape = array.shape\n",
    "    if current_shape == target_shape:\n",
    "        return array\n",
    "    \n",
    "    pad_height = target_shape[0] - current_shape[0]\n",
    "    pad_width = target_shape[1] - current_shape[1]\n",
    "    \n",
    "    if pad_height < 0 or pad_width < 0:\n",
    "        raise ValueError(\"Target shape must be larger than the current shape in both dimensions.\")\n",
    "    \n",
    "    padding = ((0, pad_height), (0, pad_width))\n",
    "    padded_array = np.pad(array, padding, mode='constant', constant_values=0)\n",
    "    \n",
    "    return padded_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for id_num, group in train_df.groupby('id_num'):\n",
    "    max_shape = (30, 30)\n",
    "    \n",
    "    result_dict[id_num] = {\n",
    "        'train': [],\n",
    "        'test': None\n",
    "    }\n",
    "    for _, row in group.iterrows():\n",
    "        input_array = parse_array(row['input'])\n",
    "        output_array = parse_array(row['output'])\n",
    "        \n",
    "        if input_array.shape != max_shape:\n",
    "            input_array = apply_padding(input_array, max_shape)\n",
    "        if output_array.shape != max_shape:\n",
    "            output_array = apply_padding(output_array, max_shape)\n",
    "\n",
    "        if 'train' in row['id']:\n",
    "            result_dict[id_num]['train'].append((input_array, output_array))\n",
    "        elif 'test' in row['id']:\n",
    "            result_dict[id_num]['test'] = (input_array, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id, task in result_dict.items():\n",
    "    print(task_id)\n",
    "    test_input = result_dict[task_id]['test'][0]\n",
    "    test_output = result_dict[task_id]['test'][1]\n",
    "    print(f\"Test input shape : {test_input.shape}, Test output shape : {test_output.shape}\")\n",
    "    for i in range(len(result_dict[task_id]['train'])):\n",
    "        print(f\"Test input shape : {result_dict[task_id]['train'][i][0].shape}, Test output shape : {result_dict[task_id]['train'][i][1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_model(input_shape, output_shape):\n",
    "    # Example model creation function with L2 regularization and dropout\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(output_shape[0] * output_shape[1]),\n",
    "        tf.keras.layers.Reshape(output_shape)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "def preprocess_data(x, y):\n",
    "    x = np.expand_dims(x, axis=0).astype(np.float32)\n",
    "    y = np.expand_dims(y, axis=0).astype(np.float32)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAML arch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAML algorithm involves two main phases: the inner loop and the outer loop.\n",
    "\n",
    "**Inner Loop:** Adapt the model parameters to a specific task.\n",
    "<br>\n",
    "**Outer Loop:** Update the initial model parameters based on how well the adapted models perform on their respective tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAML:\n",
    "    def __init__(self, inner_lr, outer_lr):\n",
    "        self.inner_lr = inner_lr\n",
    "        self.outer_lr = outer_lr\n",
    "        self.optimizer = None\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "    def adapt(self, model, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = model(x, training=True)\n",
    "            loss = mse_loss(y, preds)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        adapted_vars = [v - self.inner_lr * g for v, g in zip(model.trainable_variables, grads)]\n",
    "        for var, adapted_var in zip(model.trainable_variables, adapted_vars):\n",
    "            var.assign(adapted_var)\n",
    "        return model\n",
    "\n",
    "    def meta_train_step(self, tasks):\n",
    "        meta_grads = None\n",
    "        task_losses = {}\n",
    "        total_train_loss = 0\n",
    "        num_train_samples = 0\n",
    "        \n",
    "        for task_id, task in tasks.items():\n",
    "            input_shape = task['train'][0][0].shape\n",
    "            output_shape = task['train'][0][1].shape\n",
    "            # print(f\"Task: {task_id}, Input shape: {input_shape}, Output shape: {output_shape}\")\n",
    "\n",
    "            model = create_regression_model(input_shape, output_shape)\n",
    "            self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.outer_lr)  # Recreate optimizer\n",
    "\n",
    "            task_train_losses = []\n",
    "            for x_train, y_train in task['train']:\n",
    "                x_train, y_train = preprocess_data(x_train, y_train)\n",
    "                # print(f\"Training data shape: {x_train.shape}, {y_train.shape}\")\n",
    "                model = self.adapt(model, x_train, y_train)\n",
    "\n",
    "                preds_train = model(x_train, training=False)\n",
    "                train_loss = mse_loss(y_train, preds_train).numpy()\n",
    "                task_train_losses.append(train_loss)\n",
    "\n",
    "            avg_task_train_loss = sum(task_train_losses) / len(task_train_losses)\n",
    "            total_train_loss += avg_task_train_loss * len(task['train'])\n",
    "            num_train_samples += len(task['train'])\n",
    "\n",
    "            x_test, y_test = task['test']\n",
    "            x_test, y_test = preprocess_data(x_test, y_test)\n",
    "            # print(f\"Test data shape: {x_test.shape}, {y_test.shape}\")\n",
    "            \n",
    "            preds = model(x_test, training=False)\n",
    "            loss = mse_loss(y_test, preds).numpy()\n",
    "            task_losses[task_id] = loss\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                preds = model(x_test, training=True)\n",
    "                loss = mse_loss(y_test, preds)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "            if meta_grads is None:\n",
    "                meta_grads = grads\n",
    "            else:\n",
    "                meta_grads = [mg + g for mg, g in zip(meta_grads, grads)]\n",
    "\n",
    "        avg_train_loss = total_train_loss / num_train_samples\n",
    "        self.train_losses.append(avg_train_loss)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(meta_grads, model.trainable_variables))\n",
    "        self.test_losses.append(sum(task_losses.values()) / len(task_losses))\n",
    "        return task_losses\n",
    "\n",
    "    def train(self, tasks, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            task_losses = self.meta_train_step(tasks)\n",
    "            for task_id, loss in task_losses.items():\n",
    "                print(f\"Task: {task_id}, Loss: {loss:.4f}\")\n",
    "\n",
    "        # Plotting the losses\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(epochs), self.train_losses, label='Training Loss')\n",
    "        plt.plot(range(epochs), self.test_losses, label='Testing Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Testing Loss over Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MAML instance\n",
    "maml = MAML(inner_lr=0.001, outer_lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "maml.train(result_dict, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
